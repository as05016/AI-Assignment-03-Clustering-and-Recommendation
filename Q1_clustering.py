#!/usr/bin/env python
# coding: utf-8

# -*- coding: utf-8 -*-
"""
CS 351 - Artificial Intelligence 
Assignment 3

Altaf Shaikh - as05016:
Maheen Anees - ma05156:

"""

"""
Note: The code in the function "clusters" generates a PDF with the name "Q1_visuals.pdf" which has
      all the visuals of each iteration when visuals = True. It is recommended that when checking
      part 4 - our own Gaussian distribution parameters, run that part only and not others to avoid
      too many visuals.

      Running line 293 and 294 only generates a PDF file that has visuals for N different times that k-means 
      was run and has subsequent iterations for each run of the algorithm. (For part 1, 2, 3)

      Running line 297 only generates a PDF file that has visuals for three different set of points with
      different mean and variance. First set is the default given. Second and Third set have different mean
      and variances. 


    Also there is a warning generated by matplotlib when running line _ and _. This is because of too many
    visuals being generated and hence expected. 
"""

# Imports needed for the code
import random
import math
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# PDF initialized for storing the visuals
pp = PdfPages('Q1_visuals.pdf')


# Function to generate random data points with three different mean and variances
def initializePoints(count):
     points = []
     for i in range(int(count/3)):
         points.append([random.gauss(0,10),random.gauss(100,10)])
     for i in range(int(count/3)):
         points.append([random.gauss(-30,20),random.gauss(10,10)])
     for i in range(int(count/3)):
         points.append([random.gauss(30,20),random.gauss(10,10)])

     return points


# Helper function to calculate euclidean distance between two point tuples
def euclidean_distance(P1: tuple, P2: tuple):
     return math.sqrt(sum([(a - b) ** 2 for a, b in zip(P1, P2)]))


# Code for calculating the cluster using the K-means algorithm
def cluster(points,K,visuals = True):
    clusters=[]

    # Initially choosing K random centroids from the data points
    centroids = []
    centroid_dict = dict()
    for i in range(K):
        point = tuple(points[random.randint(0, len(points) - 1)])
        centroids.append(point)
        centroid_dict[i] = list()

    # Plotting the initial state of the data points if visuals = True
    if(visuals == True):
        Iteration = 0

        x_vals = list()
        y_vals = list()

        for j in points:
            x_vals.append(j[0])
            y_vals.append(j[1])

        fig = plt.figure()
        plt.scatter(x_vals, y_vals, alpha = 0.5, edgecolor = 'w')
        txt = f'Iteration {Iteration}'
        plt.text(0.35,0.95,txt, transform=fig.transFigure, size=24)

        for i in range(K):
            plt.scatter(centroids[i][0],centroids[i][1],marker =",",edgecolor='k',color='black')

        plt.xlabel('X axis')
        plt.ylabel('Y axis')
        pp.savefig(fig)

    # Looping until not significant change in position of centroids
    while True:

        for p in range(len(points)):
            distances = list()

            # Calculating distances of each point from each centroid
            for c in range(len(centroids)):
                d = euclidean_distance(tuple(points[p]), centroids[c])
                distances.append(d)

            # Assigning data points to their respective centroid according to min distance
            min_index = distances.index(min(distances))
            centroid_dict[min_index].append(tuple(points[p]))

        k_means = []

        # Calculating new centroids using mean of each of the K clusters
        for i in range(K):
            x_sum = 0
            y_sum = 0

            for j in centroid_dict[i]:
                x_sum += j[0]
                y_sum += j[1]
            
            if(len(centroid_dict[i]) != 0):
                x_mean = x_sum / len(centroid_dict[i])
                y_mean = y_sum / len(centroid_dict[i])
            else:
                x_mean = 0
                y_mean = 0

            k_means.append((x_mean, y_mean))


        # Updating centroids if there is significant change
        changed_K = 0
        for i in range(K):
            cent_prev = centroids[i]
            cent_next = k_means[i]
            
            d = euclidean_distance(cent_prev, cent_next)

            if(round(d) > 1):
                centroids[i] = k_means[i]
                changed_K += 1


        # Plotting interim states after changing of centroids 
        if(visuals == True):
            Iteration += 1
            
            fig = plt.figure()
            txt = f'Iteration {Iteration}'
            plt.text(0.35,0.95,txt, transform=fig.transFigure, size=24)

            for i in range(K):
                x_vals = list()
                y_vals = list()

                for j in centroid_dict[i]:
                    x_vals.append(j[0])
                    y_vals.append(j[1])

                plt.scatter(x_vals, y_vals, alpha = 0.5, edgecolor = 'w')
                plt.scatter(centroids[i][0],centroids[i][1],marker =",",edgecolor='k',color='black')

            plt.xlabel('X axis')
            plt.ylabel('Y axis')
            pp.savefig(fig)

        # If no centroids changed due to insignificant changes, we break from the loop
        if(changed_K == 0):
            break

        centroid_dict = dict()
        for i in range(K):
            centroid_dict[i] = list()

    # Returning centroids and the centroid dictionary that contains each clusters' associated data points
    clusters.append(centroids)
    clusters.append(centroid_dict)
    
    return clusters


# This function calculate the quality of a cluster using sum of squared errors. The less the value the 
# better a set of clusters.
def clusterQuality(clusters):
    
    centroids = clusters[0]
    centroid_dict = clusters[1]

    sum_sq_err = 0

    # Calculating and summing the square of distances. 
    for i in range(K):
        c = centroids[i]
        
        for p in centroid_dict[i]:
            sum_sq_err += ((euclidean_distance(c, p))**2)

    score = sum_sq_err

    return round(score, 2)
    

# Runs the k-means algorithm N times and chooses the best clustering achieved according to the score 
# in the clusterQuality function.  
def keepClustering(points,K,N,visuals):

    # Visuals are kept false in the function call as they run k-means N times - too many visuals 
    # generated in PDF. But it can be changed to True to see all visuals for each iteration.

    clusters = []
    clusters_list = []
    scores_list = []
    
    for i in range(N):

        # Adding to figure the number of each iteration, if visuals = True
        if (visuals == True):
            fig = plt.figure()
            plt.axis("off")
            txt = f'Running K-means for N = {i + 1}'
            plt.text(0.05,0.55,txt, transform=fig.transFigure, size=12)
            pp.savefig(fig) 

        # Generating cluster, getting score, and appending to scores_list
        cluster_generated = cluster(points, K, visuals)
        clusters_list.append(cluster_generated)
        score = clusterQuality(cluster_generated)
        scores_list.append(score)
    
    # Choosing the minimum (best) cluster distribution
    min_score_index = scores_list.index(min(scores_list))
    clusters = clusters_list[min_score_index]

    centroids = clusters[0]
    centroid_dict = clusters[1]            
    
    print(f'The best clustering score is for iteration number -> {min_score_index + 1}')
    return clusters


# Function to run the k-means algorithm on the default set given and two other sets of points
# One with mean = 10 and variance = 20 and other with mean = 20 and variance = 10
def newGaussClusters(K, count):
    points = []
    points = initializePoints(count)

    fig = plt.figure()
    plt.axis("off")
    txt = "Set 1 - Default set given in assignment"
    plt.text(0.05,0.55,txt, transform=fig.transFigure, size=12)
    pp.savefig(fig)

    clusters = cluster(points, K, visuals=True)

    # One can also give their own mean and variance here
    mean = 10
    variance = 20

    points = []

    fig = plt.figure()
    plt.axis("off")
    txt = f'Set 2 - Mean = {mean}, Variance = {variance}'
    plt.text(0.05,0.55,txt, transform=fig.transFigure, size=12)
    pp.savefig(fig)

    for i in range(count):
        points.append([random.gauss(mean, variance), random.gauss(mean, variance)])

    clusters = cluster(points, K, visuals=True)

    mean = 20
    variance = 10

    points = []

    fig = plt.figure()
    plt.axis("off")
    txt = f'Set 3 - Mean = {mean}, Variance = {variance}'
    plt.text(0.05,0.55,txt, transform=fig.transFigure, size=12)
    pp.savefig(fig)

    for i in range(count):
        points.append([random.gauss(mean, variance), random.gauss(mean, variance)])

    clusters = cluster(points, K, visuals=True)

K = 3
N = 10
points = initializePoints(1000)

# Either run the below two lines
clusters = keepClustering(points,K,N,True)
print ("The score of best Kmeans clustering is:", clusterQuality(clusters))

# Or run the below single line to run k-means on three different sets of points (currently commented)
#newGaussClusters(K, 1000)

# Closing the pdf file
pp.close()